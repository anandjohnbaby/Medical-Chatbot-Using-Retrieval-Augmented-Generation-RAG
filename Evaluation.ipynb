{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fd583d-51a5-415f-8c8b-e3bfa5a666e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import evaluate\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38e9a25-d9ce-4a2b-8dc8-83f80aee811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/medquad.csv\")  # Ensure the dataset has 'question' and 'answer' columns\n",
    "\n",
    "# Load embedding model\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# File paths for saved embeddings and FAISS index\n",
    "EMBEDDINGS_PATH = \"question_embeddings.npy\"\n",
    "FAISS_INDEX_PATH = \"faiss_index.bin\"\n",
    "\n",
    "# Load or compute embeddings\n",
    "if os.path.exists(EMBEDDINGS_PATH) and os.path.exists(FAISS_INDEX_PATH):\n",
    "    print(\"Loading saved FAISS index and embeddings...\")\n",
    "    question_embeddings = np.load(EMBEDDINGS_PATH)\n",
    "    index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "else:\n",
    "    print(\"Computing embeddings and creating FAISS index...\")\n",
    "    question_embeddings = np.array([embed_model.encode(q) for q in df[\"question\"]])\n",
    "    np.save(EMBEDDINGS_PATH, question_embeddings)\n",
    "\n",
    "    index = faiss.IndexFlatL2(question_embeddings.shape[1])\n",
    "    index.add(question_embeddings)\n",
    "    faiss.write_index(index, FAISS_INDEX_PATH)\n",
    "\n",
    "print(\"FAISS index ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97f15b-3918-4a52-9724-31c1b7c97c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare ground truth data\n",
    "ground_truth_questions = df[\"question\"].tolist()\n",
    "ground_truth_answers = df[\"answer\"].tolist()\n",
    "\n",
    "# Compute embeddings for ground truth answers\n",
    "ground_truth_embeddings = np.array([embed_model.encode(q) for q in ground_truth_questions])\n",
    "\n",
    "# Function to retrieve the closest question from FAISS\n",
    "def retrieve_faiss_answer(user_question):\n",
    "    user_embedding = np.array([embed_model.encode(user_question)])\n",
    "    _, closest_idx = index.search(user_embedding, 1)\n",
    "    return ground_truth_answers[closest_idx[0][0]]\n",
    "\n",
    "# Function to evaluate retrieval accuracy\n",
    "def evaluate_retrieval():\n",
    "    correct_retrievals = 0\n",
    "    total = len(ground_truth_questions)\n",
    "\n",
    "    for i, question in enumerate(ground_truth_questions):\n",
    "        retrieved_answer = retrieve_faiss_answer(question)\n",
    "        if retrieved_answer == ground_truth_answers[i]:  # Exact match\n",
    "            correct_retrievals += 1\n",
    "\n",
    "    accuracy = correct_retrievals / total\n",
    "    print(f\"Retrieval Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Function to evaluate semantic similarity\n",
    "def evaluate_similarity():\n",
    "    similarities = []\n",
    "\n",
    "    for i, question in enumerate(ground_truth_questions):\n",
    "        retrieved_answer = retrieve_faiss_answer(question)\n",
    "\n",
    "        # Handle missing or invalid answers\n",
    "        if not isinstance(retrieved_answer, str) or pd.isna(retrieved_answer):\n",
    "            print(f\"Skipping invalid answer at index {i}\")\n",
    "            continue\n",
    "\n",
    "        # Compute similarity\n",
    "        sim_score = util.pytorch_cos_sim(\n",
    "            embed_model.encode(retrieved_answer, convert_to_tensor=True),\n",
    "            embed_model.encode(ground_truth_answers[i], convert_to_tensor=True)\n",
    "        )\n",
    "\n",
    "        similarities.append(sim_score.item())\n",
    "\n",
    "    # Compute and display average similarity\n",
    "    if similarities:\n",
    "        avg_similarity = np.mean(similarities)\n",
    "        print(f\"Average Semantic Similarity: {avg_similarity:.4f}\")\n",
    "    else:\n",
    "        print(\"No valid similarities computed.\")\n",
    "\n",
    "# Function to evaluate BLEU & ROUGE\n",
    "def evaluate_nlg():\n",
    "    bleu = evaluate.load(\"bleu\")\n",
    "    rouge = evaluate.load(\"rouge\")\n",
    "\n",
    "    generated_responses = [retrieve_faiss_answer(q) for q in ground_truth_questions]\n",
    "\n",
    "    bleu_score = bleu.compute(predictions=generated_responses, references=ground_truth_answers)\n",
    "    rouge_score = rouge.compute(predictions=generated_responses, references=ground_truth_answers)\n",
    "\n",
    "    print(f\"BLEU Score: {bleu_score['bleu']:.4f}\")\n",
    "    print(f\"ROUGE Score: {rouge_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d228350e-482e-48d9-99de-4159d966c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluations\n",
    "evaluate_retrieval()\n",
    "evaluate_similarity()\n",
    "evaluate_nlg()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
